{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NN-MEAEOcode.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "T1kTJdAcyp5i",
        "TbCQIN5aypgn"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hzj62AGyD1c"
      },
      "source": [
        "##Requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bYmymuhzdo2"
      },
      "source": [
        "!pip install -q kaggle\n",
        "## extrernal \n",
        "# !pip install split-folders\n",
        "# !pip install -q kaggle\n",
        "#!pip install wandb\n",
        "# ! pip install neptune-client==0.9.8\n",
        "# ! pip install neptune-contrib\n",
        "! pip install focal_loss\n",
        "#! pip install -q tensorflow-model-optimization\n",
        "! pip install scikit-plot\n",
        "# ! pip install tensorflow neptune-client neptune-tensorflow-keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g20wfBs4ze5g"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from numpy.random import uniform, normal, randint, random\n",
        "np.random.seed(42)\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from focal_loss import BinaryFocalLoss\n",
        "import tempfile\n",
        "import os\n",
        "import scikitplot\n",
        "import tensorflow as tf\n",
        " \n",
        "from scikitplot.metrics import plot_confusion_matrix, plot_roc\n",
        "import imgaug.augmenters as iaa\n",
        "from tensorflow import keras as kr\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from skimage import transform\n",
        "from PIL import Image as im\n",
        "import skimage\n",
        "from skimage.io import imread\n",
        "from skimage.transform import resize\n",
        "import skimage as ski\n",
        "from skimage import filters\n",
        "from skimage import exposure\n",
        "from skimage.io import imsave, imread\n",
        "from copy import deepcopy\n",
        "from numpy import abs, cos, sin, pi\n",
        "import copy\n",
        "\n",
        "\n",
        "\n",
        "import math\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "\n",
        "from glob import glob\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing import image as krs_image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import cv2  \n",
        "import seaborn as sns\n",
        "import shutil"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPdt6K77yER2"
      },
      "source": [
        "##Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_Yo8wSlz3_v"
      },
      "source": [
        "# ! mkdir ~/.kaggle\n",
        "# ! cp kaggle.json ~/.kaggle/\n",
        "# ! chmod 600 ~/.kaggle/kaggle.json\n",
        "# !kaggle datasets download -d nih-chest-xrays/data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLs8gxLxzxDG"
      },
      "source": [
        "shutil.copy(\"/content/drive/MyDrive/maskzip2.zip\", \"/content/\")\n",
        "shutil.copy(\"/content/drive/MyDrive/Data_Entry_2017.zip\", \"/content/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3HBCnxkzzkY"
      },
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile(\"/content/maskzip2.zip\",\"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"/content/Untitled Folder4/\")\n",
        "with zipfile.ZipFile(\"/content/Data_Entry_2017.zip\",\"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"/content/\")    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKSTfJUAz0m9"
      },
      "source": [
        "#Download the dataset\n",
        "! git clone https://github.com/ieee8023/covid-chestxray-dataset.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfPJ5ulM0CgQ"
      },
      "source": [
        "def read_img2(img_Path,dataframe1):\n",
        "  imx=[]\n",
        "  img_list=[]\n",
        "  path = img_Path\n",
        "  for imgx in os.listdir(path):\n",
        "\n",
        "    #print(path+imgx)\n",
        "    if imgx in dataframe1.index:\n",
        "      imx=path+imgx\n",
        "      img_list.append(imx)\n",
        "  \n",
        "  return img_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzvcrdls0FbC"
      },
      "source": [
        "def read_CSV(csv_Path):\n",
        "  df_22 = pd.read_csv(csv_Path)#,index_col=[\"filename\"])\n",
        "  return df_22"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRVBQNNp0Kef"
      },
      "source": [
        "def read_CSV2(csv_Path):\n",
        "  df_22 = pd.read_csv(csv_Path)#,index_col=[\"Image Index\"])\n",
        "  return df_22"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxk3Pm_b0Oi2"
      },
      "source": [
        "def read_img(img_Path1,img_path2,dataframe1):\n",
        "  imx=[]\n",
        "  img_list=[]\n",
        "  path1 = img_Path1\n",
        "  path2 = img_path2\n",
        "  for imgx in os.listdir(img_Path1):\n",
        "   \n",
        "    if imgx in dataframe1.index:\n",
        "\n",
        "       imx=path1+imgx\n",
        "       #shutil.copy(imx, '/content/Untitled Folder/')\n",
        "\n",
        "       img_list.append(imx)\n",
        "  # for subdir, dirs, files in os.walk(img_Path1):\n",
        "    \n",
        "  #   for file in files:\n",
        "  #     if file in dataframe1.index:\n",
        "  #       imx=subdir+\"/\"+file   \n",
        "  #       img_list.append(imx)\n",
        "\n",
        "  #       #shutil.copy(imx, '/content/Untitled Folder/')\n",
        "  \n",
        "  return img_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFoAEApo0FW8"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "cols = [\"filename\",\"age\",\"finding\",\"sex\",\"view\"]\n",
        "#unique dises in the dataset \n",
        "# ['Pneumonia/Viral/COVID-19' 'Pneumonia/Viral/SARS' 'Pneumonia'\n",
        "#  'Pneumonia/Fungal/Pneumocystis' 'Pneumonia/Bacterial/Streptococcus'\n",
        "#  'No Finding' 'Pneumonia/Bacterial/Chlamydophila'\n",
        "#  'Pneumonia/Bacterial/Klebsiella' 'Pneumonia/Bacterial/Legionella'\n",
        "#  'Pneumonia/Lipoid' 'Pneumonia/Viral/Varicella' 'Pneumonia/Bacterial'\n",
        "#  'Pneumonia/Bacterial/Mycoplasma' 'Pneumonia/Viral/Influenza'\n",
        "#  'Tuberculosis' 'Pneumonia/Viral/Influenza/H1N1' 'Pneumonia/Viral/Herpes '\n",
        "#  'COVID-19/Lipoid' 'Pneumonia/Fungal/Aspergillosis' 'Pneumonia/Aspiration'\n",
        "#  'Pneumonia/Bacterial/Nocardia' 'Pneumonia/Bacterial/Staphylococcus/MRSA']\n",
        "\n",
        "#disess to classify\n",
        "disease = [\"Pneumonia/Viral/COVID-19\"]\n",
        "disease_dictionary = {'Pneumonia/Viral/COVID-19': 0}\n",
        "# read data frame \n",
        "df_read =  read_CSV(\"/content/covid-chestxray-dataset/metadata.csv\")\n",
        "#df_read.age=tf.keras.utils.normalize(df_read.age.values,axis=-1, order=2 )[0]\n",
        "#df_read.temperature=tf.keras.utils.normalize(df_read.age.values,axis=-1, order=2 )[0]\n",
        "\n",
        "\n",
        "# bins = [0, 2, 18, 35, 65, np.inf]\n",
        "# names = ['0', '1', '2', '3', '4']\n",
        "\n",
        "# df_read['age'] = pd.cut(df_read['age'], bins, labels=names)\n",
        "\n",
        " \n",
        "\n",
        "all_df = df_read\n",
        "\n",
        "dataframe = pd.DataFrame(df_read, columns=cols)\n",
        "\n",
        "\n",
        "dataframe = dataframe.loc[dataframe['view'].isin(['AP','PA'])]\n",
        "dataframe = dataframe.loc[dataframe['age']> 0]\n",
        "#dataframe = dataframe.loc[dataframe['temperature']> 0]\n",
        "dataframe = dataframe.loc[dataframe['sex'].isin(['M','F'])]\n",
        "dataframe = dataframe.loc[dataframe['finding'].isin(disease)]\n",
        "#dataframe.finding = pd.Categorical(pd.factorize(dataframe.finding)[0])\n",
        "dataframe['finding'] = dataframe['finding'].apply(lambda x: disease_dictionary[x])\n",
        "a = {'M' : 1,'F' : 0}\n",
        "dataframe['sex'] = dataframe['sex'].map(a)\n",
        "\n",
        "       \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXqIIJZz0FSn"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "cols2 = [\"Image Index\",\"Patient Age\",\"Finding Labels\",\"Patient Gender\",\"View Position\"]\n",
        "\n",
        "\n",
        "disease2 = [\"Pneumonia\"]\n",
        "disease_dictionary2 = {'Pneumonia': 1}\n",
        "dataframe_pen =  read_CSV2(\"/content/Data_Entry_2017.csv\")\n",
        "dataframe_pen = pd.DataFrame(dataframe_pen, columns=cols2)\n",
        "\n",
        "# rename dataframe columns \n",
        "dataframe_pen.rename(columns={'Image Index' : 'filename' , 'Finding Labels':'finding','Patient Age':'age',\n",
        "                              'Patient Gender':'sex','View Position':'view',\"Patient ID\":'patientid'}\n",
        "                     , inplace=True)\n",
        "\n",
        "dataframe_pen2 = dataframe_pen.loc[dataframe_pen['view'].isin(['AP','PA'])]\n",
        "dataframe_pen2 = dataframe_pen.loc[dataframe_pen['age']> 0]\n",
        "#dataframe_pen2 = dataframe_pen.loc[dataframe_pen['temperature']> 0]\n",
        "dataframe_pen2 = dataframe_pen.loc[dataframe_pen['sex'].isin(['M','F'])]\n",
        "dataframe_pen2 = dataframe_pen.loc[dataframe_pen['finding'].isin(disease2)]\n",
        "#dataframe.finding = pd.Categorical(pd.factorize(dataframe.finding)[0])\n",
        "dataframe_pen2['finding'] = dataframe_pen2['finding'].apply(lambda x: disease_dictionary2[x])\n",
        "a = {'M' : 1,'F' : 0}\n",
        "dataframe_pen2['sex'] = dataframe_pen['sex'].map(a)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdQheHgc0FMh"
      },
      "source": [
        "import shutil, os\n",
        "\n",
        "f_dataframe = pd.concat([dataframe_pen2, dataframe], axis=0)\n",
        "f_dataframe = f_dataframe.set_index('filename')\n",
        "imf_dir1='/content/Untitled Folder4/'   \n",
        "imf_dir2=\"/content/Untitled Folder4/\"  \n",
        "pne_dir = \"/content/data2/\"\n",
        "bins = [0, 2, 18, 35, 65, np.inf]\n",
        "names = ['0', '1', '2', '3', '4']\n",
        "\n",
        "f_dataframe['age'] = pd.cut(f_dataframe['age'], bins, labels=names)\n",
        "\n",
        "### Delete some images that are to noisy or incorrect\n",
        "\n",
        "bad_Images= [\"00012834_085.png\",\n",
        "             \"00019894_038.png\",\n",
        "             \"article_river_de7471906e0011eabe5f9363acaf45c4-covid-cxr-2.png\",\n",
        "             \"aqaa062i0002-b.png\",\n",
        "             \"aqaa062i0002-a.png\",\n",
        "             \"00022192_032.png\",\n",
        "             \"00022192_038.png\",\n",
        "             \"00012094_011.png\",\n",
        "             \"00026729_000.png\",\n",
        "             \"00001437_019.png\"]\n",
        "for x in bad_Images: \n",
        "  f_dataframe.drop(x)\n",
        "msk = np.random.rand(len(f_dataframe)) < 0.8\n",
        "train_df = f_dataframe[msk]\n",
        "val_df = f_dataframe[~msk]\n",
        "\n",
        "all_img = read_img(imf_dir2,pne_dir,f_dataframe)\n",
        "train_img = read_img(imf_dir2,pne_dir,train_df)\n",
        "val_img = read_img(imf_dir2,pne_dir,val_df)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5TxvwOWyEWh"
      },
      "source": [
        "##Data Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SADGWAGW0bgD"
      },
      "source": [
        "# Create the arguments for image preprocessing\n",
        "data_gen_args = dict(\n",
        "    rescale=1. / 255,\n",
        "    horizontal_flip=True,\n",
        "    brightness_range=[0.5, 1.5],\n",
        "    channel_shift_range=50,\n",
        ")\n",
        "data_gen_args2 = dict(\n",
        "    rescale=1. / 255,\n",
        ")\n",
        "seq = iaa.Sequential([\n",
        "    iaa.Affine(scale=(1, 1.2)),\n",
        "    #iaa.GaussianBlur(sigma=(0, 0.1)),\n",
        "    iaa.Fliplr(0.5)\n",
        "])\n",
        "# Create an empty data generator\n",
        "datagen = ImageDataGenerator()\n",
        "datagen2 =  ImageDataGenerator()#preprocessing_function=seq)#.augment_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fCIgp420bdy"
      },
      "source": [
        "def flip_image(imagesrc):\n",
        "  c = random.choice([1,0]) \n",
        "  flipedimage = cv2.flip(imagesrc, c)\n",
        "\n",
        "  return flipedimage\n",
        "\n",
        " \n",
        "\n",
        "def add_light(imagesrc, gamma=1.0):\n",
        "    invGamma = 1.0 / gamma\n",
        "    table = np.array([((i / 255.0) ** invGamma) * 255\n",
        "                      for i in np.arange(0, 256)]).astype(\"uint8\")\n",
        "\n",
        "    image=cv2.LUT(imagesrc, table)\n",
        "    return image\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNQAz0qy0bbo"
      },
      "source": [
        "# define an augmentation pipeline\n",
        "aug_pipeline = iaa.Sequential([\n",
        "    #iaa.Sometimes(0.5, iaa.GaussianBlur((0, 3.0))), # apply Gaussian blur with a sigma between 0 and 3 to 50% of the images\n",
        "    # apply one of the augmentations: Dropout or CoarseDropout\n",
        "    # iaa.OneOf([\n",
        "    #     iaa.Dropout((0.01, 0.1), per_channel=0.5), # randomly remove up to 10% of the pixels\n",
        "    #     iaa.CoarseDropout((0.03, 0.15), size_percent=(0.02, 0.05), per_channel=0.2),\n",
        "    # ]),\n",
        "    # apply from 0 to 3 of the augmentations from the list\n",
        "    iaa.SomeOf((0, 3),[\n",
        "        iaa.Fliplr(1.0), # horizontally flip\n",
        "        iaa.Sometimes(0.5, iaa.CropAndPad(percent=(-0.15, 0.15))), # crop and pad 50% of the images\n",
        "        iaa.Sometimes(0.2, iaa.Affine(rotate=5)) # rotate 50% of the images\n",
        "    ])],random_order=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2RQ30pz0bYv"
      },
      "source": [
        "def b223_generator(images_list, dataframex, batch_size,train= True,prediction=False,l1=False):\n",
        "    i = 0\n",
        "    while True:\n",
        "         \n",
        "        image1 = []\n",
        "        image22 = []\n",
        "        csv = []\n",
        "        labels1 = []\n",
        "        for b in range(batch_size):\n",
        "            if i == len(images_list):\n",
        "                i = 0\n",
        "                random.shuffle(images_list)\n",
        "            image_path = images_list[i]\n",
        "            image_name = os.path.basename(image_path)\n",
        "            \n",
        "            csv_row = dataframex.loc[image_name, :]\n",
        "\n",
        "       \n",
        "          \n",
        "            image2 = krs_image.load_img(image_path,color_mode=\"grayscale\")\n",
        "            image = cv2.imread(image_path,cv2.IMREAD_GRAYSCALE)\n",
        "            image = skimage.transform.resize(image,\n",
        "                               (256,256),\n",
        "                               mode='edge',\n",
        "                               anti_aliasing=False,\n",
        "                               anti_aliasing_sigma=None,\n",
        "                               preserve_range=True,\n",
        "                               order=0)\n",
        "           \n",
        "             \n",
        "            image = np.array(image, dtype=np.uint8)\n",
        "            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "            \n",
        "            image = clahe.apply(image)       \n",
        "            image = aug_pipeline.augment_image(image)\n",
        "   \n",
        "            label = csv_row['finding']\n",
        "            csv_features = csv_row.drop(['finding', 'view'])\n",
        "           \n",
        "\n",
        "            image1.append(image)\n",
        "            \n",
        "            image22.append(image)\n",
        "            \n",
        "            csv.append(csv_features)\n",
        "            labels1.append(label)\n",
        "\n",
        "            i += 1\n",
        "\n",
        "        NCOMPONENTS = 20\n",
        "         \n",
        "        image1 = np.array(image1).astype('uint8')\n",
        "        image1 = (image1 - np.min(image1))/np.ptp(image1)\n",
        "        image22 = np.array(image22).astype('uint8')\n",
        "        image22 = (image22 - np.min(image22))/np.ptp(image22)\n",
        "\n",
        "        csv= np.array(csv).astype('float32')\n",
        "     \n",
        "        labels1 = kr.utils.to_categorical(labels1, num_classes=2)\n",
        "    \n",
        "        if prediction== True:\n",
        "          yield [image1, image22 , csv]\n",
        "        if l1== True:\n",
        "          labels1\n",
        "        else:\n",
        "          yield [image1 ,image22 , csv], labels1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Af3v9NOS0bWR"
      },
      "source": [
        "def b224_generator(images_list, dataframex, batch_size,train= True,prediction=False,l1=False):\n",
        "    i = 0\n",
        "    while True:\n",
        "         \n",
        "        image1 = []\n",
        "        image22 = []\n",
        "        csv = []\n",
        "        labels1 = []\n",
        "        for b in range(batch_size):\n",
        "            if i == len(images_list):\n",
        "                i = 0\n",
        "                random.shuffle(images_list)\n",
        "            image_path = images_list[i]\n",
        "            image_name = os.path.basename(image_path)\n",
        "            \n",
        "            csv_row = dataframex.loc[image_name, :]\n",
        "\n",
        "       \n",
        "          \n",
        "            image2 = krs_image.load_img(image_path,color_mode=\"grayscale\")\n",
        "            image = cv2.imread(image_path,cv2.IMREAD_GRAYSCALE)\n",
        "            image = skimage.transform.resize(image,\n",
        "                               (256,256),\n",
        "                               mode='edge',\n",
        "                               anti_aliasing=False,\n",
        "                               anti_aliasing_sigma=None,\n",
        "                               preserve_range=True,\n",
        "                               order=0)\n",
        "           \n",
        "            # image = image.astype(np.float32) /255\n",
        "            image = np.array(image, dtype=np.uint8)\n",
        "\n",
        "            #image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "            \n",
        "            image = clahe.apply(image)         \n",
        "            #image = aug_pipeline.augment_image(image)\n",
        "   \n",
        "            label = csv_row['finding']\n",
        "            csv_features = csv_row.drop(['finding', 'view'])\n",
        "           \n",
        "\n",
        "            image1.append(image)\n",
        "            \n",
        "            image22.append(image)\n",
        "            \n",
        "            csv.append(csv_features)\n",
        "            labels1.append(label)\n",
        "\n",
        "            i += 1\n",
        "\n",
        "\n",
        "         \n",
        "        image1 = np.array(image1).astype('uint8')\n",
        "        image1 = (image1 - np.min(image1))/np.ptp(image1)\n",
        "        image22 = np.array(image22).astype('uint8')\n",
        "        image22 = (image22 - np.min(image22))/np.ptp(image22)\n",
        "\n",
        "        csv= np.array(csv).astype('float32')\n",
        "     \n",
        "        labels1 = kr.utils.to_categorical(labels1, num_classes=2)\n",
        "    \n",
        "        if prediction== True:\n",
        "          yield [image1, image22 , csv]\n",
        "        if l1== True:\n",
        "          labels1\n",
        "        else:\n",
        "          yield [image1 ,image22 , csv], labels1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVIOg6CbyEaf"
      },
      "source": [
        "##NN-Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkPMM9FyFcl2"
      },
      "source": [
        "def f_model2(): \n",
        "  activ_Func= kr.activations.relu\n",
        "  input1 =kr.layers.Input(shape=(256 , 256, 1))\n",
        "  input2 =kr.layers.Input(shape=(256 , 256, 1))\n",
        "  input3 =kr.layers.Input(shape=(2,))\n",
        "\n",
        "  x_a = kr.layers.Conv2D(filters=32,kernel_size=(1,1),  activation=activ_Func, padding='same')(input1)\n",
        "\n",
        "  x_a_1 = kr.layers.Conv2D(filters=32,kernel_size=(3,3),  activation=activ_Func, padding='same')(x_a)\n",
        "  x_a_1 = kr.layers.BatchNormalization()(x_a_1)\n",
        "  x_a_1 = kr.layers.MaxPooling2D(pool_size=3, padding='same')(x_a_1)\n",
        "  x_a_1 = kr.layers.Conv2D(filters=64,kernel_size=(3,3),  activation=activ_Func, padding='same')(x_a_1)\n",
        "  x_a_1 = kr.layers.BatchNormalization()(x_a_1)\n",
        "  x_a_1 = kr.layers.MaxPooling2D(pool_size=3, padding='same')(x_a_1)\n",
        "  x_a_1 = kr.layers.Conv2D(filters=128,kernel_size=(5,5),  activation=activ_Func, padding='same')(x_a_1)\n",
        "  x_a_1 = kr.layers.BatchNormalization()(x_a_1)\n",
        "  x_a_1 = kr.layers.MaxPooling2D(pool_size=5, padding='same')(x_a_1)\n",
        "\n",
        "\n",
        "  x_a_2 = kr.layers.Conv2D(filters=32,kernel_size=(3,3),  activation=activ_Func, padding='same')(x_a)\n",
        "  x_a_2 = kr.layers.BatchNormalization()(x_a_2)\n",
        "  x_a_2 = kr.layers.MaxPooling2D(pool_size=3, padding='same')(x_a_2)\n",
        "  x_a_2 = kr.layers.Conv2D(filters=64,kernel_size=(3,3),  activation=activ_Func, padding='same')(x_a_2)\n",
        "  x_a_2 = kr.layers.BatchNormalization()(x_a_2)\n",
        "  x_a_2 = kr.layers.MaxPooling2D(pool_size=3, padding='same')(x_a_2)\n",
        "  x_a_2 = kr.layers.Conv2D(filters=128,kernel_size=(5,5),  activation=activ_Func, padding='same')(x_a_2)\n",
        "  x_a_2 = kr.layers.BatchNormalization()(x_a_2)\n",
        "  x_a_2 = kr.layers.MaxPooling2D(pool_size=5, padding='same')(x_a_2)\n",
        "  Concatenate_a = kr.layers.Concatenate()([x_a_1, x_a_2])\n",
        "  Flatten_a = kr.layers.Flatten()(Concatenate_a)\n",
        "  Dense_a = kr.layers.Dense(128, activation=activ_Func)(Flatten_a)\n",
        "  dropout_a = kr.layers.Dropout(0.3)(Dense_a)\n",
        "  Dense_a2 = kr.layers.Dense(64, activation=activ_Func)(Dense_a)\n",
        "\n",
        "  x_b = kr.layers.Conv2D(filters=32,kernel_size=(1,1),  activation=activ_Func, padding='same')(input2)\n",
        "\n",
        "  x_b_1 = kr.layers.Conv2D(filters=32,kernel_size=(3,3),  activation=activ_Func, padding='same')(x_b)\n",
        "  x_b_1 = kr.layers.BatchNormalization()(x_b_1)\n",
        "  x_b_1 = kr.layers.MaxPooling2D(pool_size=3, padding='same')(x_b_1)\n",
        "  x_b_1 = kr.layers.Conv2D(filters=64,kernel_size=(3,3),  activation=activ_Func, padding='same')(x_b_1)\n",
        "  x_b_1 = kr.layers.BatchNormalization()(x_b_1)\n",
        "  x_b_1 = kr.layers.MaxPooling2D(pool_size=3, padding='same')(x_b_1)\n",
        "  x_b_1 = kr.layers.Conv2D(filters=64,kernel_size=(5,5),  activation=activ_Func, padding='same')(x_b_1)\n",
        "  x_b_1 = kr.layers.BatchNormalization()(x_b_1)\n",
        "  x_b_1 = kr.layers.MaxPooling2D(pool_size=5, padding='same')(x_b_1)\n",
        "\n",
        "\n",
        "  x_b_2 = kr.layers.Conv2D(filters=32,kernel_size=(3,3),  activation=activ_Func, padding='same')(x_b)\n",
        "  x_b_2 = kr.layers.BatchNormalization()(x_b_2)\n",
        "  x_b_2 = kr.layers.MaxPooling2D(pool_size=3, padding='same')(x_b_2)\n",
        "  x_b_2 = kr.layers.Conv2D(filters=64,kernel_size=(3,3),  activation=activ_Func, padding='same')(x_b_2)\n",
        "  x_b_2 = kr.layers.BatchNormalization()(x_b_2)\n",
        "  x_b_2 = kr.layers.MaxPooling2D(pool_size=3, padding='same')(x_b_2)\n",
        "  x_b_2 = kr.layers.Conv2D(filters=64,kernel_size=(5,5),  activation=activ_Func, padding='same')(x_b_2)\n",
        "  x_b_2 = kr.layers.BatchNormalization()(x_b_2)\n",
        "  x_b_2 = kr.layers.MaxPooling2D(pool_size=5, padding='same')(x_b_2)\n",
        "  Concatenate_b = kr.layers.Concatenate()([x_b_1, x_b_2])\n",
        "  Flatten_b = kr.layers.Flatten()(Concatenate_b)\n",
        "  Dense_b = kr.layers.Dense(128, activation=activ_Func)(Flatten_b)\n",
        "\n",
        "  Dense_b2 = kr.layers.Dense(64, activation=activ_Func)(Dense_b)\n",
        "\n",
        "\n",
        "  Concatenate_c = kr.layers.concatenate([Dense_a2, Dense_b2])\n",
        "  # Dense_c = kr.layers.Dense(265, activation=activ_Func)(Concatenate_c)\n",
        "  # #dropout_c = kr.layers.Dropout(0.1)(Dense_c)\n",
        "  # Dense_c2 = kr.layers.Dense(128, activation=activ_Func)(Dense_c)\n",
        "\n",
        "  ### MLP ###\n",
        "  x_mlp = kr.layers.Dense(16,activation=activ_Func)(input3)\n",
        "  x_mlp = kr.layers.Dense(8, activation=activ_Func)(x_mlp)\n",
        "  x_mlp = kr.layers.Dense(2, activation=\"linear\")(x_mlp)\n",
        "\n",
        "\n",
        "  ### Final ###\n",
        "  Concatenate2 = kr.layers.Concatenate()([x_mlp, Concatenate_c])\n",
        "  Dense1 = kr.layers.Dense(64, activation=activ_Func)(Concatenate2)\n",
        "  Dense1 = kr.layers.Dense(128, activation=activ_Func)(Dense1)\n",
        "  Dense2 = kr.layers.Dense(2, activation=\"softmax\")(Dense1)\n",
        "  Model_c = kr.models.Model(inputs=[input1,input2,input3], outputs=Dense2)\n",
        "  lr_schedule = kr.optimizers.schedules.ExponentialDecay(\n",
        "      initial_learning_rate=1e-3,\n",
        "      decay_steps=900,\n",
        "      decay_rate=0.5)\n",
        "  opt = kr.optimizers.Adam(learning_rate=lr_schedule)# ,beta_1=0.9)\n",
        "  Model_c.compile(loss =BinaryFocalLoss(gamma=2), optimizer = opt, metrics=METRICS)\n",
        "  return Model_c "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCKQkbKq0juJ"
      },
      "source": [
        "# def f_model3(): \n",
        "#   activ_Func= kr.activations.relu\n",
        "#   input1 =kr.layers.Input(shape=(256 , 256, 1))\n",
        "#   input2 =kr.layers.Input(shape=(256 , 256, 1))\n",
        "#   input3 =kr.layers.Input(shape=(2,))\n",
        "# #######################################################################################################\n",
        "#   x_a = kr.layers.Conv2D(filters=32,kernel_size=(1,1),  activation=activ_Func, padding='same')(input1)\n",
        "# #######################################################################################################\n",
        "#   x_a_1 = kr.layers.Conv2D(filters=32,kernel_size=(3,3),  activation=activ_Func, padding='same')(x_a)\n",
        "#   x_a_1 = kr.layers.MaxPooling2D(pool_size=2, padding='same')(x_a_1)\n",
        "#   x_a_1 = kr.layers.Conv2D(filters=64,kernel_size=(3,3),  activation=activ_Func, padding='same')(x_a_1)\n",
        "#   x_a_1 = kr.layers.MaxPooling2D(pool_size=2, padding='same')(x_a_1)\n",
        "#   x_a_1 = kr.layers.Conv2D(filters=64,kernel_size=(5,5),  activation=activ_Func, padding='same')(x_a_1)\n",
        "#   x_a_1 = kr.layers.MaxPooling2D(pool_size=2, padding='same')(x_a_1)\n",
        "# #######################################################################################################\n",
        "#   x_a_2 = kr.layers.Conv2D(filters=32,kernel_size=(3,3),  activation=activ_Func, padding='same')(x_a)\n",
        "#   x_a_2 = kr.layers.MaxPooling2D(pool_size=2, padding='same')(x_a_2)\n",
        "#   x_a_2 = kr.layers.Conv2D(filters=64,kernel_size=(3,3),  activation=activ_Func, padding='same')(x_a_2)\n",
        "#   x_a_2 = kr.layers.MaxPooling2D(pool_size=2, padding='same')(x_a_2)\n",
        "#   x_a_2 = kr.layers.Conv2D(filters=64,kernel_size=(5,5),  activation=activ_Func, padding='same')(x_a_2)\n",
        "#   x_a_2 = kr.layers.MaxPooling2D(pool_size=2, padding='same')(x_a_2)\n",
        "#   Concatenate_a = kr.layers.Concatenate()([x_a_1, x_a_2])\n",
        "#   Flatten_a = kr.layers.Flatten()(Concatenate_a)\n",
        "#   branchA =kr.models.Model(inputs=input1, outputs=Flatten_a)\n",
        "# #######################################################################################################\n",
        "# #######################################################################################################\n",
        "# #######################################################################################################\n",
        "# #######################################################################################################\n",
        "#   x_b = kr.layers.Conv2D(filters=32,kernel_size=(1,1),  activation=activ_Func, padding='same')(input2)\n",
        "# #######################################################################################################\n",
        "#   x_b_1 = kr.layers.Conv2D(filters=32,kernel_size=(3,3),  activation=activ_Func, padding='same')(x_b)\n",
        "#   x_b_1 = kr.layers.MaxPooling2D(pool_size=2, padding='same')(x_b_1)\n",
        "#   x_b_1 = kr.layers.Conv2D(filters=64,kernel_size=(3,3),  activation=activ_Func, padding='same')(x_b_1)\n",
        "#   x_b_1 = kr.layers.MaxPooling2D(pool_size=2, padding='same')(x_b_1)\n",
        "#   x_b_1 = kr.layers.Conv2D(filters=64,kernel_size=(5,5),  activation=activ_Func, padding='same')(x_b_1)\n",
        "#   x_b_1 = kr.layers.MaxPooling2D(pool_size=2, padding='same')(x_b_1)\n",
        "# #######################################################################################################\n",
        "#   x_b_2 = kr.layers.Conv2D(filters=32,kernel_size=(3,3),  activation=activ_Func, padding='same')(x_b)\n",
        "#   x_b_2 = kr.layers.MaxPooling2D(pool_size=2, padding='same')(x_b_2)\n",
        "#   x_b_2 = kr.layers.Conv2D(filters=64,kernel_size=(3,3),  activation=activ_Func, padding='same')(x_b_2)\n",
        "#   x_b_2 = kr.layers.MaxPooling2D(pool_size=2, padding='same')(x_b_2)\n",
        "#   x_b_2 = kr.layers.Conv2D(filters=64,kernel_size=(5,5),  activation=activ_Func, padding='same')(x_b_2)\n",
        "#   x_b_2 = kr.layers.MaxPooling2D(pool_size=2, padding='same')(x_b_2)\n",
        "#   concatenate_b = kr.layers.Concatenate()([x_b_1, x_b_2])\n",
        "#   Flatten_b = kr.layers.Flatten()(concatenate_b)\n",
        "\n",
        "#   branchB = kr.models.Model(inputs=input2, outputs=Flatten_b)\n",
        "# #######################################################################################################\n",
        "\n",
        "\n",
        "\n",
        " \n",
        " \n",
        "\n",
        "#   ### MLP ###\n",
        "#   x_mlp = kr.layers.Dense(16,activation=activ_Func)(input3)\n",
        "#   x_mlp = kr.layers.Dense(8, activation=activ_Func)(x_mlp)\n",
        "#   x_mlp = kr.layers.Dense(2, activation=\"linear\")(x_mlp)\n",
        "#   branchC = kr.models.Model(inputs=input3, outputs=x_mlp)\n",
        "\n",
        "#   combined =  kr.layers.concatenate([branchA.output, branchB.output,branchC.output])\n",
        "#   ### Final ###\n",
        "#   #Concatenate2 = kr.layers.Concatenate()([x_mlp, Dense_c2])\n",
        "#   Dense1 = kr.layers.Dense(32, activation=activ_Func)(combined)\n",
        "#   Dense1 = kr.layers.Dense(32, activation=activ_Func)(Dense1)\n",
        "#   Dense2 = kr.layers.Dense(2, activation=\"softmax\")(Dense1)\n",
        "#   Model_c = kr.models.Model(inputs=[input1,input2,input3], outputs=Dense2)\n",
        "  \n",
        "#   lr_schedule = kr.optimizers.schedules.ExponentialDecay(\n",
        "#       initial_learning_rate=1e-3,\n",
        "#       decay_steps=100,\n",
        "#       decay_rate=0.9)\n",
        "#   opt = kr.optimizers.Adam(learning_rate=0.0001)# ,beta_1=0.9)\n",
        "#   #Model_c.compile(loss =BinaryFocalLoss(gamma=2), optimizer = opt, metrics=METRICS)\n",
        "#   Model_c.compile(loss = kr.losses.binary_crossentropy, optimizer = opt, metrics=METRICS)\n",
        "#   return Model_c "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-YuBrJg0oOI"
      },
      "source": [
        "METRICS = [\n",
        "      kr.metrics.TruePositives(name='tp'),\n",
        "      kr.metrics.FalsePositives(name='fp'),\n",
        "      kr.metrics.TrueNegatives(name='tn'),\n",
        "      kr.metrics.FalseNegatives(name='fn'), \n",
        "      kr.metrics.BinaryAccuracy(name='accuracy'),\n",
        "      kr.metrics.Precision(name='precision'),\n",
        "      kr.metrics.Recall(name='recall'),\n",
        "      kr.metrics.AUC(name='auc'),\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TohD--ri0qKO"
      },
      "source": [
        "kr.backend.clear_session()\n",
        "basemodel = f_model2()\n",
        "batch_size=8\n",
        "train_steps = len(train_df)//batch_size\n",
        "val_steps =  len(val_df)//batch_size\n",
        "\n",
        "\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint('model-.h5', \n",
        "                                                               verbose=1,\n",
        "                                                               monitor='val_accuracy',\n",
        "                                                               save_best_only=True,\n",
        "                                                               mode='auto')\n",
        "\n",
        "a = b223_generator(train_img,train_df,batch_size,train=True,prediction=False,l1=False)\n",
        "b = b224_generator(val_img,val_df, batch_size,train=False,prediction=False,l1=False)\n",
        "basemodel.fit(a,\n",
        "          epochs=500,\n",
        "          steps_per_epoch=train_steps,\n",
        "          validation_data=(b),validation_steps=val_steps,callbacks=[model_checkpoint_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNyUmuoTyEeD"
      },
      "source": [
        "##Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1kTJdAcyp5i"
      },
      "source": [
        "###Objective function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gw5ZdoOb031P"
      },
      "source": [
        "def objv5(solution=None ,OP_layer=None):\n",
        "  #kr.backend.clear_session()\n",
        "  weights_matrix = []\n",
        "  start = 0\n",
        "  \n",
        "  #modelx.load_weights('beforeEAEO.h5')\n",
        "  kr.backend.clear_session()\n",
        "  model_obj =kr.models.load_model(\"my_model\")\n",
        "  ##enumerate model layers and find the required layers to optimize \n",
        "  for layer_idx, layer in enumerate(model_obj.layers):\n",
        "    if isinstance(layer, kr.layers.Conv2D) and layer.trainable :\n",
        "      if OP_layer.name == layer.name:\n",
        "         \n",
        "        layer_weights = layer.get_weights()\n",
        "        for l_weights in layer_weights:\n",
        "            layer_weights_shape = l_weights.shape\n",
        "            layer_weights_size = l_weights.size\n",
        "\n",
        "            layer_weights_vector = solution[start:start + layer_weights_size]\n",
        "            layer_weights_matrix = np.reshape(layer_weights_vector, newshape=(layer_weights_shape))\n",
        "            weights_matrix.append(layer_weights_matrix)\n",
        "            \n",
        "            start = start + layer_weights_size\n",
        "           \n",
        "        model_obj.get_layer(layer.name).set_weights(weights_matrix)\n",
        "        print(layer.name)\n",
        "        print(OP_layer.name)\n",
        "  \n",
        " \n",
        "  y_pred = model_obj.predict(x_train2) \n",
        "  y_pred = np.around(y_pred)\n",
        "  ca = kr.metrics.Accuracy()\n",
        "  ca.update_state(y_train2, y_pred)\n",
        "  accuracy = ca.result().numpy()\n",
        "  print(\"temp Fitness: {}\".format(accuracy))\n",
        "  return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbCQIN5aypgn"
      },
      "source": [
        "###ROOT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GU4psntN1zed"
      },
      "source": [
        "def setweights( solution=None):\n",
        "  weights_matrix = []\n",
        "\n",
        "  start = 0\n",
        "  for layer_idx, layer in enumerate(model.layers): # model.get_weights():\n",
        "\n",
        "   if isinstance(layer, kr.layers.Conv2D):\n",
        "      layer_weights = layer.get_weights()\n",
        "      if layer.trainable:\n",
        "          for l_weights in layer_weights:\n",
        "              layer_weights_shape = l_weights.shape\n",
        "              layer_weights_size = l_weights.size\n",
        "      \n",
        "              layer_weights_vector = solution[start:start + layer_weights_size]\n",
        "              layer_weights_matrix = np.reshape(layer_weights_vector, newshape=(layer_weights_shape))\n",
        "              weights_matrix.append(layer_weights_matrix)\n",
        "              \n",
        "              start = start + layer_weights_size\n",
        "          layer.set_weights(weights_matrix)\n",
        "          weights_matrix = []\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rk3WgzpP12ta"
      },
      "source": [
        "#!/usr/bin/env python\n",
        "# ------------------------------------------------------------------------------------------------------%\n",
        "# Created by \"Thieu Nguyen\" at 08:58, 16/03/2020                                                        %\n",
        "#                                                                                                       %\n",
        "#       Email:      nguyenthieu2102@gmail.com                                                           %\n",
        "#       Homepage:   https://www.researchgate.net/profile/Thieu_Nguyen6                                  %\n",
        "#       Github:     https://github.com/thieu1995                                                  %\n",
        "# -------------------------------------------------------------------------------------------------------%\n",
        "\n",
        "from numpy import where, clip, logical_and, maximum, minimum, power, sin, abs, pi, sqrt, sign, ones, ptp, min, sum, array, ceil, multiply, mean\n",
        "# from numpy.random import uniform, random, normal, choice\n",
        "from math import gamma\n",
        "from copy import deepcopy\n",
        "\n",
        "\n",
        "class Root:\n",
        "  \"\"\" This is root of all Algorithms \"\"\"\n",
        "\n",
        "  ID_MIN_PROB = 0  # min problem\n",
        "  ID_MAX_PROB = -1  # max problem\n",
        "\n",
        "  ID_POS = 0  # Position\n",
        "  ID_FIT = 1  # Fitness\n",
        "\n",
        "  EPSILON = 10E-10\n",
        "\n",
        "  DEFAULT_BATCH_IDEA = False\n",
        "  DEFAULT_BATCH_SIZE = 10\n",
        "  DEFAULT_LB = -1\n",
        "  DEFAULT_UB = 1\n",
        "\n",
        "  def __init__(self, obj_func=None, lb=None, ub=None, verbose=True, kwargs=None):\n",
        "      \"\"\"\n",
        "      Parameters\n",
        "      ----------\n",
        "      obj_func : function\n",
        "      lb : list\n",
        "      ub : list\n",
        "      verbose : bool\n",
        "      \"\"\"\n",
        "      if kwargs is None:\n",
        "          kwargs = {}\n",
        "      self.verbose = verbose\n",
        "      self.obj_func = obj_func\n",
        "      self.__check_parameters__(lb, ub, kwargs)\n",
        "      self.__check_optional_parameters__(kwargs)\n",
        "      self.epoch, self.pop_size = None, None\n",
        "      self.solution, self.loss_train = None, []\n",
        "\n",
        "  def __check_parameters__(self, lb, ub, kwargs):\n",
        "      if (lb is None) or (ub is None):\n",
        "          if \"problem_size\" in kwargs:\n",
        "              print(f\"Default lb={self.DEFAULT_LB}, ub={self.DEFAULT_UB}.\")\n",
        "              self.problem_size = self.__check_problem_size__(kwargs[\"problem_size\"])\n",
        "              self.lb = self.DEFAULT_LB * ones(self.problem_size)\n",
        "              self.ub = self.DEFAULT_UB * ones(self.problem_size)\n",
        "          else:\n",
        "              print(\"If lb, ub are undefined, then you must set problem size to be an integer.\")\n",
        "              exit(0)\n",
        "      else:\n",
        "          if isinstance(lb, list) and isinstance(ub, list):\n",
        "              if len(lb) == len(ub):\n",
        "                  if len(lb) == 0:\n",
        "                      if \"problem_size\" in kwargs:\n",
        "                          print(f\"Default lb={self.DEFAULT_LB}, ub={self.DEFAULT_UB}.\")\n",
        "                          self.problem_size = self.__check_problem_size__(kwargs[\"problem_size\"])\n",
        "                          self.lb = self.DEFAULT_LB * ones(self.problem_size)\n",
        "                          self.ub = self.DEFAULT_UB * ones(self.problem_size)\n",
        "                      else:\n",
        "                          print(\"Wrong lower bound and upper bound parameters.\")\n",
        "                          exit(0)\n",
        "                  elif len(lb) == 1:\n",
        "                      if \"problem_size\" in kwargs:\n",
        "                          self.problem_size = self.__check_problem_size__(kwargs[\"problem_size\"])\n",
        "                          self.lb = lb[0] * ones(self.problem_size)\n",
        "                          self.ub = ub[0] * ones(self.problem_size)\n",
        "                  else:\n",
        "                      self.problem_size = len(lb)\n",
        "                      self.lb = array(lb)\n",
        "                      self.ub = array(ub)\n",
        "              else:\n",
        "                  print(\"Lower bound and Upper bound need to be same length\")\n",
        "                  exit(0)\n",
        "          elif type(lb) in [int, float] and type(ub) in [int, float]:\n",
        "              self.problem_size = self.__check_problem_size__(kwargs[\"problem_size\"])\n",
        "              self.lb = lb * ones(self.problem_size)\n",
        "              self.ub = ub * ones(self.problem_size)\n",
        "          else:\n",
        "              print(\"Lower bound and Upper bound need to be a list.\")\n",
        "              exit(0)\n",
        "\n",
        "  def __check_problem_size__(self, problem_size):\n",
        "      if problem_size is None:\n",
        "          print(\"Problem size must be an int number\")\n",
        "          exit(0)\n",
        "      elif problem_size <= 0:\n",
        "          print(\"Problem size must > 0\")\n",
        "          exit(0)\n",
        "      return int(ceil(problem_size))\n",
        "\n",
        "  def __check_optional_parameters__(self, kwargs):\n",
        "      if \"batch_idea\" in kwargs:\n",
        "          batch_idea = kwargs[\"batch_idea\"]\n",
        "          if type(batch_idea) == bool:\n",
        "              self.batch_idea = batch_idea\n",
        "          else:\n",
        "              self.batch_idea = self.DEFAULT_BATCH_IDEA\n",
        "          if \"batch_size\" in kwargs:\n",
        "              batch_size = kwargs[\"batch_size\"]\n",
        "              if type(batch_size) == int:\n",
        "                  self.batch_size = batch_size\n",
        "              else:\n",
        "                  self.batch_size = self.DEFAULT_BATCH_SIZE\n",
        "          else:\n",
        "              self.batch_size = self.DEFAULT_BATCH_SIZE\n",
        "      else:\n",
        "          self.batch_idea = self.DEFAULT_BATCH_IDEA\n",
        "\n",
        "  def create_solution(self, minmax=0):\n",
        "      \"\"\" Return the position position with 2 element: position of position and fitness of position\n",
        "\n",
        "      Parameters\n",
        "      ----------\n",
        "      minmax\n",
        "          0 - minimum problem, else - maximum problem\n",
        "\n",
        "      \"\"\"\n",
        "      position = uniform(self.lb, self.ub)\n",
        "      fitness = self.get_fitness_position(position=position, minmax=minmax)\n",
        "      return [position, fitness]\n",
        "\n",
        "  def get_fitness_position(self, position=None,layertooptimize=None):\n",
        "      \"\"\"     Assumption that objective function always return the original value\n",
        "      :param position: 1-D numpy array\n",
        "      :param minmax: 0- min problem, 1 - max problem\n",
        "      :return:\n",
        "      \"\"\"\n",
        "      return self.obj_func(position,layertooptimize) #if minmax == 0 else 1.0 / (self.obj_func(position) + self.EPSILON)\n",
        "  def get_fitness_position2(self, position=None,layertooptimize=None):\n",
        "      \"\"\"     Assumption that objective function always return the original value\n",
        "      :param position: 1-D numpy array\n",
        "      :param minmax: 0- min problem, 1 - max problem\n",
        "      :return:\n",
        "      \"\"\"\n",
        "      return self.obj_func(position,layertooptimize,) #if minmax == 0 else 1.0 / (self.obj_func(position) + self.EPSILON)     \n",
        "\n",
        "  def get_fitness_solution(self, solution=None, minmax=0):\n",
        "      return self.get_fitness_position(solution[self.ID_POS], minmax)\n",
        "\n",
        "  def get_global_best_solution(self, pop=None, id_fit=None, id_best=None):\n",
        "      \"\"\" Sort a copy of population and return the copy of the best position \"\"\"\n",
        "      sorted_pop = sorted(pop, key=lambda temp: temp[id_fit])\n",
        "      return deepcopy(sorted_pop[id_best])\n",
        "\n",
        "  def get_global_best_global_worst_solution(self, pop=None, id_fit=None, id_best=None):\n",
        "      sorted_pop = sorted(pop, key=lambda temp: temp[id_fit])\n",
        "      if id_best == self.ID_MIN_PROB:\n",
        "          return deepcopy(sorted_pop[id_best]), deepcopy(sorted_pop[self.ID_MAX_PROB])\n",
        "      elif id_best == self.ID_MAX_PROB:\n",
        "          return deepcopy(sorted_pop[id_best]), deepcopy(sorted_pop[self.ID_MIN_PROB])\n",
        "\n",
        "  def update_global_best_global_worst_solution(self, pop=None, id_best=None, id_worst=None, g_best=None):\n",
        "      \"\"\" Sort the copy of population and update the current best position. Return the new current best position \"\"\"\n",
        "      sorted_pop = sorted(pop, key=lambda temp: temp[self.ID_FIT])\n",
        "      current_best = sorted_pop[id_best]\n",
        "      g_best = deepcopy(current_best) if current_best[self.ID_FIT] < g_best[self.ID_FIT] else deepcopy(g_best)\n",
        "      return g_best, sorted_pop[id_worst]\n",
        "\n",
        "  def get_sorted_pop_and_global_best_solution(self, pop=None, id_fit=None, id_best=None):\n",
        "      \"\"\" Sort population and return the sorted population and the best position \"\"\"\n",
        "      sorted_pop = sorted(pop, key=lambda temp: temp[id_fit])\n",
        "      return sorted_pop, deepcopy(sorted_pop[id_best])\n",
        "\n",
        "  def amend_position(self, position=None):\n",
        "      return maximum(self.lb, minimum(self.ub, position))\n",
        "\n",
        "  def amend_position_faster(self, position=None):\n",
        "      return clip(position, self.lb, self.ub)\n",
        "\n",
        "  def activ_ReLu(self, position=None):\n",
        "      return maximum(0,position)\n",
        "\n",
        "  def amend_position_random(self, position=None):\n",
        "      for t in range(self.problem_size):\n",
        "          if position[t] < self.lb[t] or position[t] > self.ub[t]:\n",
        "              position[t] = uniform(self.lb[t], self.ub[t])\n",
        "      return position\n",
        "\n",
        "  def amend_position_random_faster(self, position=None):\n",
        "      return where(logical_and(self.lb <= position, position <= self.ub), position, uniform(self.lb, self.ub))\n",
        "\n",
        "  def update_global_best_solution(self, pop=None, id_best=None, g_best=None):\n",
        "      \"\"\" Sort the copy of population and update the current best position. Return the new current best position \"\"\"\n",
        "      sorted_pop = sorted(pop, key=lambda temp: temp[self.ID_FIT])\n",
        "      current_best = sorted_pop[id_best]\n",
        "      return deepcopy(current_best) if current_best[self.ID_FIT] < g_best[self.ID_FIT] else deepcopy(g_best)\n",
        "\n",
        "  def update_sorted_population_and_global_best_solution(self, pop=None, id_best=None, g_best=None):\n",
        "      \"\"\" Sort the population and update the current best position. Return the sorted population and the new current best position \"\"\"\n",
        "      sorted_pop = sorted(pop, key=lambda temp: temp[self.ID_FIT])\n",
        "      current_best = sorted_pop[id_best]\n",
        "      g_best = deepcopy(current_best) if current_best[self.ID_FIT] < g_best[self.ID_FIT] else deepcopy(g_best)\n",
        "      return sorted_pop, g_best\n",
        "\n",
        "  def create_opposition_position(self, position=None, g_best=None):\n",
        "      return self.lb + self.ub - g_best[self.ID_POS] + uniform() * (g_best[self.ID_POS] - position)\n",
        "\n",
        "  def levy_flight(self, epoch=None, position=None, g_best_position=None, step=0.001, case=0):\n",
        "      \"\"\"\n",
        "      Parameters\n",
        "      ----------\n",
        "      epoch (int): current iteration\n",
        "      position : 1-D numpy array\n",
        "      g_best_position : 1-D numpy array\n",
        "      step (float, optional): 0.001\n",
        "      case (int, optional): 0, 1, 2\n",
        "\n",
        "      \"\"\"\n",
        "      beta = 1\n",
        "      # muy and v are two random variables which follow normal distribution\n",
        "      # sigma_muy : standard deviation of muy\n",
        "      sigma_muy = power(gamma(1 + beta) * sin(pi * beta / 2) / (gamma((1 + beta) / 2) * beta * power(2, (beta - 1) / 2)), 1 / beta)\n",
        "      # sigma_v : standard deviation of v\n",
        "      sigma_v = 1\n",
        "      muy = normal(0, sigma_muy ** 2)\n",
        "      v = normal(0, sigma_v ** 2)\n",
        "      s = muy / power(abs(v), 1 / beta)\n",
        "      levy = uniform(self.lb, self.ub) * step * s * (position - g_best_position)\n",
        "\n",
        "      if case == 0:\n",
        "          return levy\n",
        "      elif case == 1:\n",
        "          return position + 1.0 / sqrt(epoch + 1) * sign(random() - 0.5) * levy\n",
        "      elif case == 2:\n",
        "          return position + normal(0, 1, len(self.lb)) * levy\n",
        "      elif case == 3:\n",
        "          return position + 0.01 * levy\n",
        "\n",
        "  def levy_flight_2(self, position=None, g_best_position=None):\n",
        "      alpha = 0.01\n",
        "      xichma_v = 1\n",
        "      xichma_u = ((gamma(1 + 1.5) * sin(pi * 1.5 / 2)) / (gamma((1 + 1.5) / 2) * 1.5 * 2 ** ((1.5 - 1) / 2))) ** (1.0 / 1.5)\n",
        "      levy_b = (normal(0, xichma_u ** 2)) / (sqrt(abs(normal(0, xichma_v ** 2))) ** (1.0 / 1.5))\n",
        "      return position + alpha * levy_b * (position - g_best_position)\n",
        "\n",
        "  def step_size_by_levy_flight(self, multiplier=0.001, beta=1.0, case=0):\n",
        "      \"\"\"\n",
        "      Parameters\n",
        "      ----------\n",
        "      multiplier (float, optional): 0.01\n",
        "      beta: [0-2]\n",
        "          + 0-1: small range --> exploit\n",
        "          + 1-2: large range --> explore\n",
        "      case: 0, 1, -1\n",
        "          + 0: return multiplier * s * uniform()\n",
        "          + 1: return multiplier * s * normal(0, 1)\n",
        "          + -1: return multiplier * s\n",
        "      \"\"\"\n",
        "      # u and v are two random variables which follow normal distribution\n",
        "      # sigma_u : standard deviation of u\n",
        "      sigma_u = power(gamma(1 + beta) * sin(pi * beta / 2) / (gamma((1 + beta) / 2) * beta * power(2, (beta - 1) / 2)), 1 / beta)\n",
        "      # sigma_v : standard deviation of v\n",
        "      sigma_v = 1\n",
        "      u = normal(0, sigma_u ** 2)\n",
        "      v = normal(0, sigma_v ** 2)\n",
        "      s = u / power(abs(v), 1 / beta)\n",
        "      if case == 0:\n",
        "          step = multiplier * s * uniform()\n",
        "      elif case == 1:\n",
        "          step = multiplier * s * normal(0, 1)\n",
        "      else:\n",
        "          step = multiplier * s\n",
        "      return step\n",
        "\n",
        "  def get_index_roulette_wheel_selection(self, list_fitness=None):\n",
        "      \"\"\" It can handle negative also. Make sure your list fitness is 1D-numpy array\"\"\"\n",
        "      scaled_fitness = (list_fitness - min(list_fitness)) / (ptp(list_fitness) + self.EPSILON)\n",
        "      minimized_fitness = 1.0 - scaled_fitness\n",
        "      total_sum = sum(minimized_fitness)\n",
        "      r = uniform(low=0, high=total_sum)\n",
        "      for idx, f in enumerate(minimized_fitness):\n",
        "          r = r + f\n",
        "          if r > total_sum:\n",
        "              return idx\n",
        "\n",
        "  def get_parent_kway_tournament_selection(self, pop=None, k_way=0.2, output=2):\n",
        "      if 0 < k_way < 1:\n",
        "          k_way = int(k_way * len(pop))\n",
        "      list_id = choice(range(len(pop)), k_way, replace=False)\n",
        "      list_parents = [pop[i] for i in list_id]\n",
        "      list_parents = sorted(list_parents, key=lambda temp: temp[self.ID_FIT])\n",
        "      return list_parents[:output]\n",
        "\n",
        "  ### Crossover\n",
        "  def crossover_arthmetic_recombination(self, dad_pos=None, mom_pos=None):\n",
        "      r = uniform()           # w1 = w2 when r =0.5\n",
        "      w1 = multiply(r, dad_pos) + multiply((1 - r), mom_pos)\n",
        "      w2 = multiply(r, mom_pos) + multiply((1 - r), dad_pos)\n",
        "      return w1, w2\n",
        "\n",
        "  ### Mutation\n",
        "  ### This method won't be used in any algorithm because of it's slow performance\n",
        "  ### Using numpy vector for faster performance\n",
        "  def mutation_flip_point(self, parent_pos, idx):\n",
        "      w = deepcopy(parent_pos)\n",
        "      w[idx] = uniform(self.lb[idx], self.ub[idx])\n",
        "      return w\n",
        "\n",
        "\n",
        "  #### Improved techniques can be used in any algorithms: 1\n",
        "  ## Based on this paper: An efficient equilibrium optimizer with mutation strategy for numerical optimization (but still different)\n",
        "  ## This scheme used after the original and including 4 step:\n",
        "  ##  s1: sort population, take p1 = 1/2 best population for next round\n",
        "  ##  s2: do the mutation for p1, using greedy method to select the better solution\n",
        "  ##  s3: do the search mechanism for p1 (based on global best solution and the updated p1 above), to make p2 population\n",
        "  ##  s4: construct the new population for next generation\n",
        "  def improved_ms(self, pop=None, g_best=None):    ## m: mutation, s: search\n",
        "      pop_len = int(len(pop) / 2)\n",
        "      ## Sort the updated population based on fitness\n",
        "      pop = sorted(pop, key=lambda item: item[self.ID_FIT])\n",
        "      pop_s1, pop_s2 = pop[:pop_len], pop[pop_len:]\n",
        "      ## Mutation scheme\n",
        "      for i in range(0, pop_len):\n",
        "          pos_new = pop_s1[i][self.ID_POS] * (1 + normal(0, 1, self.problem_size))\n",
        "          fit = self.get_fitness_position(pos_new)\n",
        "          if fit < pop_s1[i][self.ID_FIT]:        ## Greedy method --> improved exploitation\n",
        "              pop_s1[i] = [pos_new, fit]\n",
        "      ## Search Mechanism\n",
        "      pos_s1_list = [item[self.ID_POS] for item in pop_s1]\n",
        "      pos_s1_mean = mean(pos_s1_list, axis=0)\n",
        "      for i in range(0, pop_len):\n",
        "          pos_new = (g_best[self.ID_POS] - pos_s1_mean) - random() * (self.lb + random() * (self.ub - self.lb))\n",
        "          fit = self.get_fitness_position(pos_new)\n",
        "          pop_s2[i] = [pos_new, fit]              ## Keep the diversity of populatoin and still improved the exploration\n",
        "\n",
        "      ## Construct a new population\n",
        "      pop = pop_s1 + pop_s2\n",
        "      pop, g_best = self.update_sorted_population_and_global_best_solution(pop, self.ID_MIN_PROB, g_best)\n",
        "      return pop, g_best\n",
        "\n",
        "  def train(self):\n",
        "      pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JAMokcuypYi"
      },
      "source": [
        "###Modifed EAEO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZXEsTyTWHHZ"
      },
      "source": [
        "from numpy.random import random as rn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmE2zMV81yJe"
      },
      "source": [
        "\n",
        "# ------------------------------------------------------------------------------------------------------%\n",
        "# Modifed version\"Thieu Nguyen\" at 08:58, 16/03/2020                                                        %\n",
        "#                                                                                                       %\n",
        "#       Email:      nguyenthieu2102@gmail.com                                                           %\n",
        "#       Homepage:   https://www.researchgate.net/profile/Thieu_Nguyen6                                  %\n",
        "#       Github:     https://github.com/thieu1995                                                  %\n",
        "# -------------------------------------------------------------------------------------------------------%\n",
        "\n",
        "class EAA6(Root):\n",
        "  from numpy.random import uniform, normal, randint, random\n",
        "  from numpy import abs, cos, sin, pi\n",
        "  from copy import deepcopy  \n",
        "  from numpy.random import random as rn\n",
        "\n",
        "  def __init__(self, obj_func=None, lb=None, ub=None,problem=None, verbose=True, epoch=750, pop_size=100, **kwargs):\n",
        "      super().__init__(obj_func, lb, ub, verbose, kwargs)\n",
        "      self.epoch = epoch\n",
        "      self.pop_size = pop_size\n",
        "      self.problem = problem\n",
        "      self.firstrun =True\n",
        "\n",
        "  \n",
        "  def model_weights_as_vector(self, model):\n",
        "      weights_vector = []\n",
        "\n",
        "      \n",
        "      layer_weights = model.get_weights()\n",
        "      for l_weights in layer_weights:\n",
        "          vector = np.reshape(l_weights, newshape=(l_weights.size))\n",
        "          weights_vector.extend(vector)\n",
        "\n",
        "      return np.array(weights_vector) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def create_solution2(self, sloutionx):\n",
        "     \n",
        "    if self.firstrun : \n",
        "       position = copy.deepcopy(sloutionx)\n",
        "       position = np.array(position)\n",
        "       self.firstrun=False\n",
        "    else: \n",
        "      position = copy.deepcopy(sloutionx)\n",
        "      position = np.array(position) + np.random.uniform(low=-0.5, high=0.5, size=len(sloutionx))\n",
        "\n",
        "    fitness = self.get_fitness_position2(position=position,layertooptimize = self.problem  )\n",
        "    return [position, fitness]\n",
        "\n",
        "  def train(self):\n",
        "      weights_vector = self.model_weights_as_vector(model=self.problem)\n",
        "      pop=[]\n",
        "      pop = [self.create_solution2(sloutionx = weights_vector) for _ in range(self.pop_size)]\n",
        "     # pop = [self.create_solution2(sloutionx = self.problem) for _ in range(self.pop_size)]      # Sorted population in the descending order of the function fitness value\n",
        "      pop = sorted(pop, key=lambda item: item[self.ID_FIT])#, reverse=False)\n",
        "      g_best = deepcopy(pop[self.ID_MAX_PROB])\n",
        "\n",
        "      pop_new = deepcopy(pop)\n",
        "      self.firstrun =True\n",
        "      for epoch in range(self.epoch):\n",
        "          ## Production\n",
        "          # Eq. 2\n",
        "          a = 2* (1 - (epoch+1) / self.epoch)\n",
        "          x1 = (1 - a) * pop[self.ID_MAX_PROB][self.ID_POS] + a * uniform(self.lb, self.ub)\n",
        "          fit = self.get_fitness_position(x1, self.problem)\n",
        "          pop_new[0] = [x1, fit]\n",
        "\n",
        "          ## Consumption\n",
        "          # Eq. 7 , 8 , 9\n",
        "          for i in range(2, self.pop_size):\n",
        "              rand = uniform()\n",
        "              old_position = pop[i][self.ID_POS]\n",
        "             \n",
        "             \n",
        "              v1 = normal(0, 1)\n",
        "              v2 = normal(0, 1)\n",
        "              c = 0.5 * v1 / abs(v2)  # Consumption factor\n",
        "            \n",
        "            \n",
        "              r3 = 2*pi*rn()\n",
        "              r4 = rn()\n",
        "\n",
        "              j = randint(1, i)\n",
        "              ### Herbivore\n",
        "              if rand <= 1.0 / 3:  \n",
        "                \n",
        "                  if r4 <= 0.5:\n",
        "                      x_t1 = old_position + sin(r3) * c * (old_position - pop[0][self.ID_POS])\n",
        "                  else:\n",
        "                      x_t1 = old_position + cos(r3) * c * (old_position - pop[0][self.ID_POS])\n",
        "              ### Carnivore\n",
        "              elif 1.0 / 3 <= rand and rand <= 2.0 / 3:  \n",
        "                \n",
        "                  if r4 <= 0.5:\n",
        "                      x_t1 = old_position + sin(r3) * c * (old_position - pop[j][self.ID_POS])\n",
        "                  else:\n",
        "                      x_t1 = old_position + cos(r3) * c * (old_position - pop[j][self.ID_POS])\n",
        "              ### Omnivore\n",
        "              else:             \n",
        "                \n",
        "                  r5 = rn()\n",
        "                  if r4 <= 0.5:\n",
        "                      x_t1 = old_position + sin(r5) * c * (r5 * (old_position - pop[0][self.ID_POS]) + (1 - r5) * (old_position - pop[j][self.ID_POS]))\n",
        "                  else:\n",
        "                      x_t1 = old_position + cos(r5) * c * (r5 * (old_position - pop[0][self.ID_POS]) + (1 - r5) * (old_position - pop[j][self.ID_POS]))\n",
        "              x_t1 = self.amend_position_faster(x_t1)\n",
        "              fit_t1 = self.get_fitness_position2(x_t1,layertooptimize = self.problem)\n",
        "              pop_new[i] = [x_t1, fit_t1]\n",
        "              x_t1= 0 \n",
        "          for i in range(0, self.pop_size):\n",
        "              if pop_new[i][self.ID_FIT] > pop[i][self.ID_FIT]:\n",
        "                  pop[i] = deepcopy(pop_new[i])\n",
        "\n",
        "          ## find current best used in decomposition\n",
        "          best = self.get_global_best_solution(pop, self.ID_FIT, self.ID_MAX_PROB)\n",
        "\n",
        "          ## Decomposition\n",
        "           #Eq. 10 \n",
        "          for i in range(0, self.pop_size):\n",
        "              r3 = uniform()\n",
        "              d = 3 * normal(0, 1)\n",
        "              e = r3 * randint(1, 3) - 1\n",
        "              h = 2 * r3 - 1\n",
        "              x_new = best[self.ID_POS] + d * (e * best[self.ID_POS] - h * pop[i][self.ID_POS])\n",
        "              if uniform() < 0.5:\n",
        "                  beta = 1 - (1 - 0) * ((epoch + 1) / self.epoch)  # Eq. 21\n",
        "                  x_r = pop[randint(0, self.pop_size)][self.ID_POS]\n",
        "                  if uniform() < 0.5:\n",
        "                      x_new = beta * x_r + (1 - beta) * pop[i][self.ID_POS]\n",
        "                  else:\n",
        "                      x_new = beta * pop[i][self.ID_POS] + (1 - beta) * x_r\n",
        "              else:\n",
        "                  best[self.ID_POS] = best[self.ID_POS] + normal() * best[self.ID_POS]\n",
        "              x_new = self.amend_position_faster(x_new)\n",
        "              fit_new = self.get_fitness_position2(x_new,layertooptimize = self.problem)\n",
        "              pop_new[i] = [x_new, fit_new]\n",
        "\n",
        "          for i in range(0, self.pop_size):\n",
        "              if pop_new[i][self.ID_FIT] > pop[i][self.ID_FIT]:\n",
        "                  pop[i] = deepcopy(pop_new[i])\n",
        "\n",
        "          pop = sorted(pop, key=lambda item: item[self.ID_FIT], reverse=False)\n",
        "         \n",
        "         \n",
        "          current_best = deepcopy(pop[self.ID_MAX_PROB])\n",
        "\n",
        "\n",
        "          if current_best[self.ID_FIT] > g_best[self.ID_FIT]:\n",
        "              g_best = deepcopy(current_best)\n",
        "              self.loss_train.append(g_best[self.ID_FIT])\n",
        "          if self.verbose:\n",
        "              print(\"> Epoch: {}, Best fit: {}\".format(epoch + 1, g_best[self.ID_FIT]))\n",
        "          current_best= 0\n",
        "      self.solution = g_best\n",
        "      return g_best[self.ID_POS], g_best[self.ID_FIT], self.loss_train\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ItAFD3Zy69X"
      },
      "source": [
        "##Running the  optimiztion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdSGIijmIxZx"
      },
      "source": [
        "\n",
        "basemodel.save(\"my_model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6gIrgO92Ks7"
      },
      "source": [
        "x_train2 , y_train2 = next(b223_generator(train_img,train_df,len(train_img),train=False,prediction=False,l1=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZB3YKsP72K1G"
      },
      "source": [
        "kr.backend.clear_session()\n",
        "weights_vector = []\n",
        "start = 0\n",
        "modelx = f_model2()\n",
        "modelx.load_weights('BbeforeEAEO.h5')\n",
        "modelx.save(\"my_model\")\n",
        "for layer_idx, layer in enumerate(modelx.layers): # model.get_weights():\n",
        " \n",
        "  \n",
        "\n",
        "  if isinstance(layer, kr.layers.Conv2D) and layer.trainable :\n",
        "    layer_weights = layer.get_weights()\n",
        "\n",
        "     \n",
        "    for l_weights in layer_weights:\n",
        "\n",
        "        vector = np.reshape(l_weights, newshape=(l_weights.size))\n",
        "        weights_vector.extend(vector)\n",
        "    obj_func = objv5\n",
        "    problemSize = len(weights_vector)\n",
        "\n",
        "    verbose = True\n",
        "    epoch = 5\n",
        "    pop_size = 5\n",
        "    lb = -1\n",
        "    ub = 1   \n",
        "    print(\"layer name EAEO: {}\".format(layer.name))\n",
        "    a = EAA6(obj_func, lb, ub ,layer,verbose, epoch, pop_size, problem_size=problemSize)\n",
        "    best_pos1, best_fit1, list_loss1 = a.train()   \n",
        "\n",
        "\n",
        "\n",
        "    weights_matrix = []\n",
        "    start= 0 \n",
        "    for l_weights in layer_weights:\n",
        "        layer_weights_shape = l_weights.shape\n",
        "        layer_weights_size = l_weights.size\n",
        "\n",
        "        layer_weights_vector = best_pos1[start:start + layer_weights_size]\n",
        "        layer_weights_matrix = np.reshape(layer_weights_vector, newshape=(layer_weights_shape))\n",
        "        weights_matrix.append(layer_weights_matrix)\n",
        "        \n",
        "        start = start + layer_weights_size\n",
        "    kr.backend.clear_session()\n",
        "    modelx =kr.models.load_model(\"my_model\")\n",
        "\n",
        "    modelx =  kr.models.load_model(\"my_model\") \n",
        "    modelx.get_layer(layer.name).set_weights(weights_matrix)\n",
        "    modelx.save(\"my_model\")\n",
        "    weights_matrix = []\n",
        "    weights_vector=[]\n",
        "    x_train2 , y_train2 = next(b223_generator(train_img,train_df,len(train_img),train=False,prediction=False,l1=False))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPYbqxEvzWU-"
      },
      "source": [
        "##Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQT1NniQ2eOP"
      },
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "x_train , y_true = next(b224_generator(val_img,val_df,len(val_df),train=False,prediction=False,l1=False))\n",
        "#x_train , y_true = next(b223_generator(train_img,train_df,len(train_df),train=False,prediction=False,l1=False))\n",
        "\n",
        "## before EAEO\n",
        "modelx.load_weights('/content/model-.h5')\n",
        "predictions3 =  modelx.predict(x_train)\n",
        "predictions4 = np.around(predictions3)\n",
        "target_names = ['class 0', 'class 1']\n",
        "print(classification_report(y_true, predictions4, target_names=target_names)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AU35Ew042eID"
      },
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "x_train , y_true = next(b224_generator(val_img,val_df,len(val_df),train=False,prediction=False,l1=False))\n",
        "#x_train , y_true = next(b223_generator(train_img,train_df,len(train_df),train=False,prediction=False,l1=False))\n",
        "\n",
        "## before EAEO\n",
        "#modelx.load_weights('/content/afterEAEO.h5')\n",
        "modelx =  kr.models.load_model(\"my_model\") \n",
        "\n",
        "predictions3 =  modelx.predict(x_train)\n",
        "predictions4 = np.around(predictions3)\n",
        "target_names = ['class 0', 'class 1']\n",
        "print(classification_report(y_true, predictions4, target_names=target_names)) "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
